rule_token_symbol2symbol_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.symbol;
			.symbol := ast_symbol
			{
				.data := parse_state.src # (lexeme.off);
				.len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_stash ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.stash;
	};
};

rule_unstash ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.unstash;
	};
};

rule_shift1 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 1;};
	};
};

rule_shift2 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 2;};
	};
};

rule_shift3 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 3;};
	};
};

rule_shift4 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 4;};
	};
};

rule_commit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.commit;
	};
};

rule_token_numeric_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := interpret_numeric_literal(parse_state.src, lexeme);
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_char_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.char;
				.chars := parse_state.src # (lexeme.off);
				.chars_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_string_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.string;
				.chars := parse_state.src # (lexeme.off);
				.chars_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_true2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.boolean;
				.boolean := true;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_false2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.boolean;
				.boolean := false;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_zero2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.zero_value;
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_oparen_expr_x_token_cparen2expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have ( expr )
	// literally just make it expr
	(nodes # 1)->begin_cursor = ((nodes # 0)->begin_cursor);
	(nodes # 1)->loc = ((nodes # 0)->loc);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(deref(nodes # 1));
	};
};

rule_expr_x_token_oparen2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr(
	expr_node ::= nodes # 0;
	callnode ::= ast mut
	{
		.tag := ast_tag.callwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref callnode, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(callnode);
	};
};

//rule_token_symbol_token_oparen2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
//{
//	// foo(
//	lexeme ::= (nodes # 0)->utok.tok.lexeme;
//	callnode ::= ast
//	{
//		.tag := ast_tag.callwip;
//		.call := ast_call
//		{
//			.call_name := parse_state.src # (lexeme.off);
//			.call_name_len := lexeme.len;
//		};
//		.children := zero;
//		.children_count := zero;
//		.children_cap := zero;
//	};
//	return rule_result
//	{
//		.tag := rule.reduce;
//		.reduce := reduce_replace_all_with(callnode);
//	};
//};

rule_callwip_token_cparen2call ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// callwip )
	// turn it into a call expr
	callwip_node ::= nodes # 0;
	callwip_node->tag = (ast_tag.expr);
	callwip_node->expr = (ast_expr_tag.call);
	callwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_callwip_expr_x2expect_comma_or_cparen ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// callwip expr
	callwip_node ::= nodes # 0;
	callwip_node->tag = (ast_tag.call_expect_comma_or_cparen);
	expr_node ::= nodes # 1;
	ast_add_child(callwip_node, deref expr_node, global_arena);
	callwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_call_expect_comma_or_cparen2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// got a call_expect_comma_or_cparen by itself
	// but the lookahead is a cparen
	// instead of shifting it we just turn it back into a callwip now and re-use some existing rules.
	callwip_expect_comma_or_cparen_node ::= nodes # 0;
	callwip_expect_comma_or_cparen_node->tag = (ast_tag.callwip);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_call_expect_comma_or_cparen_token_comma2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// got a call_expect_comma_or_cparen,
	// means we put it back into wipcall
	callwip_expect_comma_or_cparen_node ::= nodes # 0;
	callwip_expect_comma_or_cparen_node->tag = (ast_tag.callwip);
	callwip_expect_comma_or_cparen_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_call_comma_followed_by_cparen ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	call_node ::= nodes # 0;
	builtin_name_sym ::= ast_get_child(call_node, 0)->symbol;
	psyc_error_begin((nodes # 1)->loc);
	putzstr("in call to ");
	putchar('"');
	putbytes(builtin_name_sym.data, builtin_name_sym.len);
	putchar('"');
	putzstr(", a ',' must be followed by another parameter");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 1, nodes_count - 1, underline_colour.red);
	parse_print_state();
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_token_obrack_expr_token_cbrack2deref ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have [ expr ]
	// make it into a deref expr
	expr_node ::= deref(nodes # 1);
	deref_node ::= ast mut
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.dereference;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref deref_node, expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(deref_node);
	};
};

//rule_token_x_expr2unop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
//{
//	// x expr
//	// where x is some token
//	// we don't know directly which unop type it is so we will read it from an array.
//	// we figure it out from a magic 'ast_unop_token_mapping' array
//	token_node ::= nodes # 0;
//	expr_node ::= nodes # 1;
//	tok ::= token_node->utok.tok.tok;
//	unop ::= (lex_token_as_operator # (tok@s64))->unop;
//	if(unop == zero)
//	{
//		psyc_panic_begin(token_node->loc);
//		putzstr("lex token ");
//		putzstr(__enumname(tok));
//		putzstr(" did not map to a valid ast_unop via 'ast_unop_token_mapping'. please submit a bug report.");
//		psyc_diag_end();
//		psyc_exit_failure();
//	}
//
//	unop_node ::= ast
//	{
//		.tag := ast_tag.expr;
//		.expr := ast_expr_tag.unop;
//		.unop := unop;
//		.children := zero;
//		.children_count := zero;
//		.children_cap := zero;
//	};
//	ast_add_child(ref unop_node, deref expr_node, global_arena);
//	return rule_result
//	{
//		.tag := rule.reduce;
//		.reduce := reduce_replace_all_with(unop_node);
//	};
//};

rule_token_x_expr_x2unop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we've got a token of some type
	// we expect it to be a valid unop
	token_node ::= nodes # 0;
	expr_node ::= nodes # 1;

	tok ::= token_node->utok.tok.tok;

	op : ast_unop mut := zero;
	// make sure the token is a valid unop
	i : u64 mut;
	for(i = 0, i < __countof(ast_unop), i = i + 1)
	{
		if(deref(ast_unop_tokens # (i + 1)) == tok)
		{
			op = ((i + 1)@ast_unop);
		}
	}
	if(op == zero)
	{
		psyc_panic_begin(token_node->loc);
		putzstr("lex_token.");
		putzstr(__enumname(tok));
		putzstr(" is not a valid unary operator");
		psyc_diag_end();
		psyc_exit_failure();
	}
	unop_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.unop;
		.unop := op;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref unop_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(unop_node);
	};
};

rule_expr_x_token_x_expr_y2biop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we've got a token of some type
	// we expect it to be a valid biop
	lhs_expr_node ::= nodes # 0;
	token_node ::= nodes # 1;
	rhs_expr_node ::= nodes # 2;

	tok ::= token_node->utok.tok.tok;

	op : ast_biop mut := zero;
	// make sure the token is a valid biop
	i : u64 mut;
	for(i = 0, i < __countof(ast_biop), i = i + 1)
	{
		if(deref(ast_biop_tokens # (i + 1)) == tok)
		{
			op = ((i + 1)@ast_biop);
		}
	}
	if(op == zero)
	{
		psyc_panic_begin(token_node->loc);
		putzstr("lex_token.");
		putzstr(__enumname(tok));
		putzstr(" is not a valid binary operator");
		psyc_diag_end();
		psyc_exit_failure();
	}
	biop_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.biop;
		.biop := op;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref biop_node, deref lhs_expr_node, global_arena);
	ast_add_child(ref biop_node, deref rhs_expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(biop_node);
	};
};

rule_token_symbol2typename_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	name ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.typename;
			.typename := ast_typename
			{
				.tag := ast_typename_tag.wip;
				.quals := zero;
				.descriptor := ast_typename_descriptor.base;
				.basename := parse_state.src # (name.off);
				.basename_len := name.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_typename_wip2mut ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.qual_mut));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.qual_mut to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2static ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.qual_static));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.qual_static to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2weak ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.qual_weak));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.qual_weak to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2ptr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// make the current first node an underlying type of a new typename node (pointer)
	underlying ::= deref(nodes # 0);
	ptr_node ::= ast mut
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.wip;
			.quals := zero;
			.descriptor := ast_typename_descriptor.pointer;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref ptr_node, underlying, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ptr_node);
	};
};

rule_typename_wip2array_await_length ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	underlying ::= deref(nodes # 0);
	array_node ::= ast mut
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.array_await_length;
			.quals := zero;
			.descriptor := ast_typename_descriptor.array;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref array_node, underlying, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(array_node);
	};
};

rule_typename_array_await_length_expr_token_semicol2wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	array_length_expr ::= deref(nodes # 1);
	// add the array length as another child (typename_node should already have a child which is the underlying type)
	// then set the tag to wip (incase it has more stuff afterwards)...
	ast_add_child(typename_node, array_length_expr, global_arena);
	typename_node->typename.tag = (ast_typename_tag.wip);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_keyword_sizeof_typename2sizeof ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// sizeof typename
	tyname_node ::= nodes # 1;
	expr_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.sizeof_t;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref expr_node, deref tyname_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(expr_node);
	};
};

rule_keyword_alignof_typename2alignof ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// alignof typename
	tyname_node ::= nodes # 1;
	expr_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.alignof_t;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref expr_node, deref tyname_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(expr_node);
	};
};

rule_keyword_countof_typename2countof ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// countof typename
	tyname_node ::= nodes # 1;
	expr_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.countof_t;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref expr_node, deref tyname_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(expr_node);
	};
};

rule_keyword_func_keyword_oparen2function_await_param_or_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// func(
	// make it into a function typename
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.typename;
			.typename := ast_typename
			{
				.tag := ast_typename_tag.function_await_param_or_arrow;
				.quals := zero;
				.descriptor := ast_typename_descriptor.function;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_typename_function_await_param_or_arrow_decl_finalised_token_arrow2function_await_return_type ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a function stem with a last param + an arrow
	// add the param decl as a child and then set tag to function_await_return_type
	typename_node ::= nodes # 0;
	decl_node ::= nodes # 1;
	ast_add_child(typename_node, deref decl_node, global_arena);
	typename_node->typename.tag = (ast_typename_tag.function_await_return_type);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_function_await_param_or_arrow_decl_finalised_token_comma2function_await_param_or_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a function stem with a param and then a comma
	// add the param decl as a child and then set tag to function_await_param_or_arrow
	// we obviously dont expect an arrow next but thats handled by an erroneous-case grammar_rule that has the arrow as a lookahead - in other words we dont deal with that error case in this rule.
	typename_node ::= nodes # 0;
	decl_node ::= nodes # 1;
	ast_add_child(typename_node, deref decl_node, global_arena);
	typename_node->typename.tag = (ast_typename_tag.function_await_param_or_arrow);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_typename_function_comma_followed_by_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	psyc_error_begin((nodes # 2)->loc);
	putzstr("in function typename, a ',' must be followed by another typename ");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 2, nodes_count - 2, underline_colour.red);
	parse_print_state();
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_typename_function_await_param_or_arrow_token_arrow2await_return_type ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_function_await_param_or_arrow arrow
	// make put it in the 'await return type' tag
	typename_node ::= nodes # 0;
	typename_node->typename.tag = (ast_typename_tag.function_await_return_type);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_function_await_return_type_typename_token_cparen2wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_function_await_return_type typename_finalised cparen
	// i.e a full function stem

	// we move it to wip instead of finalised (as function typenames can have qualifiers etc...)
	typename_node ::= nodes # 0;
	return_typename_node ::= nodes # 1;
	typename_node->typename.tag = (ast_typename_tag.wip);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	ast_add_child(typename_node, deref return_typename_node, global_arena);
	
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	// make the typename finalised.
	typename_node->typename.tag = (ast_typename_tag.finalised);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_keyword_typeof_expr_x2typename ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typeof expr_x
	// turn into typename
	expr_node ::= nodes # 1;
	typename_node ::= ast mut
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.wip;
			.quals := zero;
			.descriptor := ast_typename_descriptor.inferred_from;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref typename_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(typename_node);
	};
};

rule_token_symbol_token_colon2decl_awaittype ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	name ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.decl;
			.decl := ast_decl
			{
				.tag := ast_decl_tag.await_type;
				.name := parse_state.src # (name.off);
				.name_len := name.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_decl_awaittype_typename_finalised2noinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// have a decl that awaits a typename
	// and a finalised typename
	// it's now a well-formed decl but without an init. we cant say the decl itself is finalised because we dont know if an initialisre is coming next so we will just say noinit for now.
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.noinit);
	ast_add_child(decl_node, deref(nodes # 1), global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_decl_noinit_token_initialiser2await_init_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// have a decl without an init followed by a :=
	// change its state to await_init_expr
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.await_init_expr);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_decl_noinit2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a decl that is marked noinit
	// but it definitely isnt followed by an initialiser
	// we just mark it as finalised
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.finalised);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon_token_initialiser2decl_await_init_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we are 'foo ::='
	// we want to convert this to a decl with 'deduced' descriptor typename and the 'await_init_expr' tag
	// this is the same as 'foo : xyz :='
	// so we can share that code
	name ::= (nodes # 0)->utok.tok.lexeme;
	new_decl_node : ast mut := ast
	{
		.tag := ast_tag.decl;
		.decl := ast_decl
		{
			.tag := ast_decl_tag.await_init_expr;
			.name := parse_state.src # (name.off);
			.name_len := name.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	initialiser_node ::= (nodes # 2);
	// this is the typename
	ast_add_child(ref new_decl_node, ast
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.finalised;
			.descriptor := ast_typename_descriptor.deduced;
		};
		.begin_cursor := initialiser_node->begin_cursor;
		.end_cursor := initialiser_node->end_cursor;
		.loc := initialiser_node->loc;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	}, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(new_decl_node);
	};
};

rule_decl_await_init_expr_init_expr2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a decl that awaits an init expr
	// followed by a finalised expr that the grammar says is a valid initialiser
	// shove it as a child and call it a day
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.finalised);
	ast_add_child(decl_node, deref(nodes # 1), global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_keyword_return_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.stmt;
			.stmt := ast_stmt_tag.ret;
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_return_expr_x_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// return expr_x;
	expr_node ::= nodes # 1;
	ret_node ::= ast
	{
		.tag := ast_tag.stmt;
		.stmt := ast_stmt_tag.ret;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref ret_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ret_node);
	};
};

rule_token_symbol_token_colon_token_initialiser_token_keyword_struct_token_obrace2structure_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a bunch of tokens
	// turn them into an empty struct so we can stash it and add members
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.structure;
			.structure := ast_structure
			{
				.tag := ast_structure_tag.wip;
				.name := parse_state.src # (lexeme.off);
				.name_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_structure_wip_token_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a struct_wip and a cbrace
	// just set the struct as finalised and swallow everything else
	structure_node ::= nodes # 0;
	structure_node->structure.tag = (ast_structure_tag.finalised);
	structure_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon_token_initialiser_token_keyword_enum_token_obrace2enumeration_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a bunch of tokens
	// turn them into an empty enum so we can stash it and add members
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.enumeration;
			.enumeration := ast_enumeration
			{
				.tag := ast_enumeration_tag.wip;
				.name := parse_state.src # (lexeme.off);
				.name_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_enumeration_wip_token_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a enumeration_wip and a cbrace
	// just set the enum as finalised and swallow everything else
	enumeration_node ::= nodes # 0;
	enumeration_node->enumeration.tag = (ast_enumeration_tag.finalised);
	enumeration_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};


rule_decl_finalised_token_obrace2function_impl ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// decl_finalised {
	// this is the syntax for a function definition
	// we check the typename of the decl and ensure that is a function type
	decl_node ::= nodes # 0;
	typename_node ::= ast_get_child(decl_node, 0);
	if(typename_node->tag != (ast_tag.typename))
	{
		psyc_panic_begin(decl_node->loc);
		putzstr("internal error: first child of decl node was expected to be ast_tag.typename but it is ast_tag.");
		putzstr(__enumname(typename_node->tag));
		psyc_diag_end();
		psyc_exit_failure();
	}
	typename ::= typename_node->typename;
	if(typename.descriptor != (ast_typename_descriptor.function)) 
	{
		psyc_error_begin(decl_node->loc);
		putzstr("decl followed immediately by a '{' means a function definition, so i expect the decl typename to be a function type, but it is a ");
		putzstr(__enumname(typename.descriptor));
		putzstr(" type");
		psyc_diag_end();
		psyc_exit_failure();
	}
	decl ::= decl_node->decl;

	function_node ::= ast
	{
		.tag := ast_tag.function;
		.function := ast_function
		{
			.tag := ast_function_tag.impl;
			.name := decl.name;
			.name_len := decl.name_len;
			.is_extern := false;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref function_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(function_node);
	};
};

rule_decl_finalised_token_keyword_extern2function_finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// decl_finalised extern
	// this is the syntax for a function definition (extern)
	// we check the typename of the decl and ensure that is a function type
	decl_node ::= nodes # 0;
	typename_node ::= ast_get_child(decl_node, 0);
	if(typename_node->tag != (ast_tag.typename))
	{
		psyc_panic_begin(decl_node->loc);
		putzstr("internal error: first child of decl node was expected to be ast_tag.typename but it is ast_tag.");
		putzstr(__enumname(typename_node->tag));
		psyc_diag_end();
		psyc_exit_failure();
	}
	typename ::= typename_node->typename;
	if(typename.descriptor != (ast_typename_descriptor.function)) 
	{
		psyc_error_begin(decl_node->loc);
		putzstr("decl followed immediately by a '{' means a function definition, so i expect the decl typename to be a function type, but it is a ");
		putzstr(__enumname(typename.descriptor));
		putzstr(" type");
		psyc_diag_end();
		psyc_exit_failure();
	}
	decl ::= decl_node->decl;

	function_node ::= ast
	{
		.tag := ast_tag.function;
		.function := ast_function
		{
			.tag := ast_function_tag.finalised;
			.name := decl.name;
			.name_len := decl.name_len;
			.is_extern := true;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref function_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(function_node);
	};
};

rule_typename_finalised_token_obrace2blkinitwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_finalised{
	// create a blkinitwip
	typename_node ::= nodes # 0;
	blkinit_node ::= ast
	{
		.tag := ast_tag.blkinitwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref blkinit_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(blkinit_node);
	};
};

rule_blkinitwip_token_cbrace2blkinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	blkinitwip_node ::= nodes # 0;
	blkinitwip_node->tag = (ast_tag.expr);
	blkinitwip_node->expr = (ast_expr_tag.blkinit);
	blkinitwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_dot_token_symbol_token_initialiser2desiginitwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// .foo := 
	// turn into desiginitwip
	lexeme ::= (nodes # 1)->utok.tok.lexeme;
	desiginit_node ::= ast
	{
		.tag := ast_tag.desiginitwip;
		.desiginit := ast_desiginit
		{
			.descriptor := ast_desiginit_descriptor.symbol;
			.symbol := parse_state.src # (lexeme.off);
			.symbol_len := lexeme.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(desiginit_node);
	};
};

rule_token_dot_token_numeric_literal_token_initialiser2desiginitwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// .5 := 
	// turn into desiginitwip
	lexeme ::= (nodes # 1)->utok.tok.lexeme;
	index ::= interpret_numeric_literal(parse_state.src, lexeme);
	if(index.tag != (ast_literal_tag.integer))
	{
		psyc_error_begin((nodes # 0)->loc);
		putzstr("a designated initialiser index must be an integral literal, you have provided a ");
		putzstr(__enumname(index.tag));
		putchar(10);
		ast_print_annotated_source(parse_state.src, nodes # 0, nodes_count, underline_colour.red);
		parse_print_state();
		psyc_diag_end();
		psyc_exit_failure();
		return zero;
	}
	desiginit_node ::= ast
	{
		.tag := ast_tag.desiginitwip;
		.desiginit := ast_desiginit
		{
			.descriptor := ast_desiginit_descriptor.integer;
			.integer := index.integer;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(desiginit_node);
	};
};

rule_desiginitwip_expr_x2desiginit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// desiginitwip expr
	// turn it into a proper desiginit
	desiginitwip_node ::= nodes # 0;
	expr_node ::= nodes # 1;
	ast_add_child(desiginitwip_node, deref expr_node, global_arena);
	desiginitwip_node->tag = (ast_tag.desiginit);
	desiginitwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_expr_x_token_cast2castwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr@
	expr_node ::= nodes # 0;
	cast_node ::= ast
	{
		.tag := ast_tag.castwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref cast_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(cast_node);
	};
};

rule_function_impl_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// function_impl }
	// means we are done and need to set our function to be in its finalised state
	function_node ::= nodes # 0;
	function_node->function.tag = (ast_function_tag.finalised);
	function_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_castwip_typename_finalised2cast ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// castwip typename_finalised
	// just add the typename_finalised as the 2nd child (first child should be the expr that we're casting)
	castwip_node ::= nodes # 0;
	typename_node ::= nodes # 1;
	if(castwip_node->children_count != 1)
	{
		psyc_panic_begin(castwip_node->loc);
		putzstr("invalid castwip state. should have 1 child (the expr to cast) but it had ");
		putuint(castwip_node->children_count);
		psyc_diag_end();
		psyc_exit_failure();
	}
	ast_add_child(castwip_node, deref typename_node, global_arena);
	castwip_node->tag = (ast_tag.expr);
	castwip_node->expr = (ast_expr_tag.cast);
	castwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_expr_x_token_dot2fieldwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr_x.
	// make into fieldwip
	expr_node ::= nodes # 0;
	field_node ::= ast
	{
		.tag := ast_tag.fieldwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref field_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(field_node);
	};
};

rule_fieldwip_expr_x2field ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// fieldwip expr
	// turn this into a proper field
	// fieldwip should already have a child 0 (the expr representing the struct/enum)
	fieldwip_node ::= nodes # 0;
	// remember this expr is expected to be either a symbol expr or a callfunc
	// in any case a symbol is easy to retrieve.
	expr_node ::= nodes # 1;
	ast_add_child(fieldwip_node, deref expr_node, global_arena);

	fieldwip_node->tag = (ast_tag.expr);
	fieldwip_node->expr = (ast_expr_tag.field);
	fieldwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};


rule_expr_x_token_arrow2ptrfieldwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr_x->
	// make into ptrfieldwip
	expr_node ::= nodes # 0;
	field_node ::= ast
	{
		.tag := ast_tag.ptrfieldwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref field_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(field_node);
	};
};

rule_ptrfieldwip_expr_x2ptrfield ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// ptrfieldwip expr
	// turn this into a proper ptrfield
	// ptrfieldwip should already have a child 0 (the expr representing the ptr)
	ptrfieldwip_node ::= nodes # 0;
	// remember this expr is expected to be either a symbol expr or a callfunc
	// in any case a symbol is easy to retrieve.
	expr_node ::= nodes # 1;
	ast_add_child(ptrfieldwip_node, deref expr_node, global_arena);

	ptrfieldwip_node->tag = (ast_tag.expr);
	ptrfieldwip_node->expr = (ast_expr_tag.ptrfield);
	ptrfieldwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_keyword_if_expr_x_token_obrace2ifwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// if expr{
	// make into ifwip
	expr_node ::= nodes # 1;

	ifwip_node ::= ast
	{
		.tag := ast_tag.ifwip;
		.ifblk := ast_if
		{
			.is_static := false;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref ifwip_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ifwip_node);
	};
};

rule_token_keyword_static_if_expr_x_token_obrace2ifwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// static_if expr{
	// make into ifwip
	expr_node ::= nodes # 1;

	ifwip_node ::= ast
	{
		.tag := ast_tag.ifwip;
		.ifblk := ast_if
		{
			.is_static := true;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref ifwip_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ifwip_node);
	};
};

rule_error_else_without_preceding_if_stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	psyc_error_begin((nodes # 0)->loc);
	putzstr("an else statement can only be valid if placed directly after an if-statement");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 0, nodes_count, underline_colour.red);
	parse_print_state();
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_token_keyword_else_token_obrace2elsewip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// else{
	// with an if stmt as stashtop
	// just return an empty wipelse...
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.elsewip;
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_elsewip_token_cbrace2else ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// elsewip}
	// with an if stmt as stashtop
	// turn into proper else stmt
	elsewip_node ::= nodes # 0;
	elsewip_node->tag = (ast_tag.stmt);
	elsewip_node->stmt = (ast_stmt_tag.else_block);
	elsewip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_ifstmt_elsestmt2ifstmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// if_stmt else_stmt
	// make else_stmt a child of the if_stmt
	// and we're done
	if_node ::= nodes # 0;
	else_node ::= nodes # 1;
	if_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	ast_add_child(if_node, deref else_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_ifwip_token_cbrace2if ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// ifwip}
	// cap off into if stmt
	ifwip_node ::= nodes # 0;
	ifwip_node->tag = (ast_tag.stmt);
	ifwip_node->stmt = (ast_stmt_tag.if_block);
	ifwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_keyword_while_expr_x_token_obrace2whilewip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// while expr{
	// make into whilewip
	expr_node ::= nodes # 1;

	whilewip_node ::= ast
	{
		.tag := ast_tag.whilewip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref whilewip_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(whilewip_node);
	};
};

rule_keyword_for_oparen2wipfor0 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// for(
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.forwip0;
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_forwip0_expr_x_token_semicol2forwip1 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip0 expr;
	// make into forwip1
	forwip_node ::= nodes # 0;
	expr_node ::= nodes # 1;
	ast_add_child(forwip_node, deref(expr_node), global_arena);
	forwip_node->tag = (ast_tag.forwip1);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_forwip1_expr_x_token_semicol2forwip2 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip1 expr;
	// make into forwip2
	forwip_node ::= nodes # 0;
	expr_node ::= nodes # 1;
	ast_add_child(forwip_node, deref(expr_node), global_arena);
	forwip_node->tag = (ast_tag.forwip2);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_forwip2_expr_x_token_cparen_token_obrace2forwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip2 expr){
	// make into forwip
	forwip_node ::= nodes # 0;
	expr_node ::= nodes # 1;
	ast_add_child(forwip_node, deref(expr_node), global_arena);
	forwip_node->tag = (ast_tag.forwip);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_whilewip_token_cbrace2while ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// whilewip}
	// cap off into while stmt
	whilewip_node ::= nodes # 0;
	whilewip_node->tag = (ast_tag.stmt);
	whilewip_node->stmt = (ast_stmt_tag.while_block);
	whilewip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

// hot DAMN SON thats a long function name
rule_token_symbol_token_colon_token_keyword_asm_token_oparen_token_string_literal_token_arrow2asmwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// symbol : asm(string_literal)
	constraints_lexeme ::= (nodes # 4)->utok.tok.lexeme;
	name_lexeme ::= (nodes # 0)->utok.tok.lexeme;
	asm_node ::= ast
	{
		.tag := ast_tag.assembly;
		.assembly := ast_assembly
		{
			.tag := ast_assembly_tag.wip;
			.name := parse_state.src # (name_lexeme.off);
			.name_len := name_lexeme.len;
			.constraints := parse_state.src # (constraints_lexeme.off);
			.constraints_len := constraints_lexeme.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(asm_node);
	};
};

rule_token_keyword_for_stmt_x2forwip1 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// for stmt_x
	// make into forwip1
	stmt_node ::= nodes # 1;
	forwip1_node ::= ast
	{
		.tag := ast_tag.forwip1;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref forwip1_node, deref stmt_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(forwip1_node);
	};
};

rule_forwip1_stmt_x2forwip2 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip1 stmt_x
	// make into forwip2
	forwip2_node ::= nodes # 0;
	forwip2_node->tag = (ast_tag.forwip2);
	stmt_node ::= nodes # 1;
	ast_add_child(forwip2_node, deref stmt_node, global_arena);
	forwip2_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_forwip2_stmt_x_token_obrace2forwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip2 stmt_x{
	// make into forwip
	forwip_node ::= nodes # 0;
	forwip_node->tag = (ast_tag.forwip);
	stmt_node ::= nodes # 1;
	ast_add_child(forwip_node, deref stmt_node, global_arena);
	forwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_forwip_token_cbrace2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// forwip }
	// make into for stmt
	forwip_node ::= nodes # 0;
	forwip_node->tag = (ast_tag.stmt);
	forwip_node->stmt = (ast_stmt_tag.for_block);
	forwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_assembly_wip_typename_finalised_token_cparen_token_obrace2coding ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// asmwip typename){
	// we are now expecting everything to be assembly code (until the next cbrace which ends it)
	asm_node ::= nodes # 0;
	asm_node->assembly.tag = (ast_assembly_tag.coding);
	asm_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);

	typename_node ::= nodes # 1;
	ast_add_child(asm_node, deref typename_node, global_arena);

	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_assembly_coding_token_x2coding ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// assembly_coding x
	// where x is any token that isnt a cbrace
	asm_node ::= nodes # 0;
	tok_node ::= nodes # 1;
	// ok so this node may or may not already have a child
	new_child ::= asm_node->children_count == 1;
	if(new_child)
	{
		ast_add_child(asm_node, deref tok_node, global_arena);
	}
	child ::= ast_get_child(asm_node, 1);
	if(!new_child)
	{
		// ok so this is a bit weird
		// tok_node lexeme may not start at the same point
		// but we need to include it all
		// so we add (tok_node.lexeme.off - child_node.lexeme.len)
		// to get to the begin of the new lexeme
		// and then also add tok_node.lexeme.len:
		child_lexeme ::= child->utok.tok.lexeme;
		new_lexeme ::= tok_node->utok.tok.lexeme;
		// increase len enough to touch new_lexeme.off
		// i.e child_lexeme.off + child_lexeme.len + n = new_lexeme.off
		// n = new_lexeme.off - (child_lexeme.off + child_lexeme.len)
		n ::= new_lexeme.off - (child_lexeme.off + (child_lexeme.len));
		child->utok.tok.lexeme.len = (child->utok.tok.lexeme.len + (n + new_lexeme.len));
	}
	child->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	asm_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_assembly_coding_token_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	asm_node ::= nodes # 0;
	asm_node->assembly.tag = (ast_assembly_tag.finalised);
	asm_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_compare_token_symbol_token_compare_token_obrace2regionwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// == symbol == {
	lexeme ::= (nodes # 1)->utok.tok.lexeme;
	regionwip_node ::= ast
	{
		.tag := ast_tag.regionwip;
		.region := ast_region
		{
			.name := parse_state.src # (lexeme.off);
			.name_len := lexeme.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(regionwip_node);
	};
};

rule_regionwip_token_cbrace_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// regionwip };
	// cap off a regionwip into a region stmt.
	region_node ::= nodes # 0;
	region_node->tag = (ast_tag.stmt);
	region_node->stmt = (ast_stmt_tag.region);
	region_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_expr_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have an expr (of any particular tag) and a semicol
	expr_node ::= nodes # 0;
	(expr_node->tag) = (ast_tag.stmt);
	(expr_node->stmt) = (ast_stmt_tag.expr);
	last ::= nodes # (nodes_count - 1);
	expr_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_function_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised function and a semicol
	function_node ::= nodes # 0;
	(function_node->tag) = (ast_tag.stmt);
	(function_node->stmt) = (ast_stmt_tag.function);
	last ::= nodes # (nodes_count - 1);
	function_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_desiginit_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a desiginit and a semicol
	desiginit_node ::= nodes # 0;
	(desiginit_node->tag) = (ast_tag.stmt);
	(desiginit_node->stmt) = (ast_stmt_tag.desiginit);
	last ::= nodes # (nodes_count - 1);
	desiginit_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_commit_non_desiginit_to_blkinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	stmt_node ::= nodes # 0;
	psyc_error_begin(stmt_node->loc);
	putzstr("the ");
	putzstr(__enumname(stmt_node->stmt));
	putzstr(" below is not valid within a block initialiser, only designated initialisers are allowed.");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 0, nodes_count, underline_colour.red);
	parse_print_state();
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_decl_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised decl and a semicol
	decl_node ::= nodes # 0;
	(decl_node->tag) = (ast_tag.stmt);
	(decl_node->stmt) = (ast_stmt_tag.decl);
	last ::= nodes # (nodes_count - 1);
	decl_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_structure_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised structure and a semicol
	structure_node ::= nodes # 0;
	(structure_node->tag) = (ast_tag.stmt);
	(structure_node->stmt) = (ast_stmt_tag.structure);
	last ::= nodes # (nodes_count - 1);
	structure_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_enumeration_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised enumeration and a semicol
	enumeration_node ::= nodes # 0;
	(enumeration_node->tag) = (ast_tag.stmt);
	(enumeration_node->stmt) = (ast_stmt_tag.enumeration);
	last ::= nodes # (nodes_count - 1);
	enumeration_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_assembly_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised assembly and a semicol
	assembly_node ::= nodes # 0;
	(assembly_node->tag) = (ast_tag.stmt);
	(assembly_node->stmt) = (ast_stmt_tag.assembly);
	last ::= nodes # (nodes_count - 1);
	assembly_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};
