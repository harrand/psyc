rule_token_symbol2symbol_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.symbol;
			.symbol := ast_symbol
			{
				.data := parse_state.src # (lexeme.off);
				.len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_stash ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.stash;
	};
};

rule_unstash ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.unstash;
	};
};

rule_shift1 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 1;};
	};
};

rule_shift2 ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.shift;
		.shift := shift_data{.count := 2;};
	};
};

rule_commit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.commit;
	};
};

rule_token_numeric_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := interpret_numeric_literal(parse_state.src, lexeme);
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_char_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.char;
				.chars := parse_state.src # (lexeme.off);
				.chars_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_string_literal2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.string;
				.chars := parse_state.src # (lexeme.off);
				.chars_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_true2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.boolean;
				.boolean := true;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_false2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.literal;
			.literal := ast_literal
			{
				.tag := ast_literal_tag.boolean;
				.boolean := false;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_keyword_zero2literal_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.expr;
			.expr := ast_expr_tag.zero_value;
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_token_oparen_expr_x_token_cparen2expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have ( expr )
	// literally just make it expr
	(nodes # 1)->begin_cursor = ((nodes # 0)->begin_cursor);
	(nodes # 1)->loc = ((nodes # 0)->loc);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(deref(nodes # 1));
	};
};

rule_token_symbol_token_oparen2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// foo(
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	callnode ::= ast
	{
		.tag := ast_tag.callwip;
		.call := ast_call
		{
			.call_name := parse_state.src # (lexeme.off);
			.call_name_len := lexeme.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(callnode);
	};
};

rule_callwip_token_cparen2call ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// callwip )
	// turn it into a call expr
	callwip_node ::= nodes # 0;
	callwip_node->tag = (ast_tag.expr);
	callwip_node->expr = (ast_expr_tag.call);
	// callwip_node->call was already populated while in its callwip state.
	callwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_callwip_expr_x2expect_comma_or_cparen ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// callwip expr
	callwip_node ::= nodes # 0;
	callwip_node->tag = (ast_tag.call_expect_comma_or_cparen);
	expr_node ::= nodes # 1;
	ast_add_child(callwip_node, deref expr_node, global_arena);
	callwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_call_expect_comma_or_cparen2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// got a call_expect_comma_or_cparen by itself
	// but the lookahead is a cparen
	// instead of shifting it we just turn it back into a callwip now and re-use some existing rules.
	callwip_expect_comma_or_cparen_node ::= nodes # 0;
	callwip_expect_comma_or_cparen_node->tag = (ast_tag.callwip);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_call_expect_comma_or_cparen_token_comma2callwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// got a call_expect_comma_or_cparen,
	// means we put it back into wipcall
	callwip_expect_comma_or_cparen_node ::= nodes # 0;
	callwip_expect_comma_or_cparen_node->tag = (ast_tag.callwip);
	callwip_expect_comma_or_cparen_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_call_comma_followed_by_cparen ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	call_node ::= nodes # 0;
	psyc_error_begin((nodes # 1)->loc);
	putzstr("in call to ");
	putchar('"');
	putbytes(call_node->call.call_name, call_node->call.call_name_len);
	putchar('"');
	putzstr(", a ',' must be followed by another parameter");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 1, nodes_count - 1);
	return zero;
};

rule_token_obrack_expr_token_cbrack2deref ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have [ expr ]
	// make it into a deref expr
	expr_node ::= deref(nodes # 1);
	deref_node ::= ast mut
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.dereference;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref deref_node, expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(deref_node);
	};
};

//rule_token_x_expr2unop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
//{
//	// x expr
//	// where x is some token
//	// we don't know directly which unop type it is so we will read it from an array.
//	// we figure it out from a magic 'ast_unop_token_mapping' array
//	token_node ::= nodes # 0;
//	expr_node ::= nodes # 1;
//	tok ::= token_node->utok.tok.tok;
//	unop ::= (lex_token_as_operator # (tok@s64))->unop;
//	if(unop == zero)
//	{
//		psyc_panic_begin(token_node->loc);
//		putzstr("lex token ");
//		putzstr(__enumname(tok));
//		putzstr(" did not map to a valid ast_unop via 'ast_unop_token_mapping'. please submit a bug report.");
//		psyc_diag_end();
//		psyc_exit_failure();
//	}
//
//	unop_node ::= ast
//	{
//		.tag := ast_tag.expr;
//		.expr := ast_expr_tag.unop;
//		.unop := unop;
//		.children := zero;
//		.children_count := zero;
//		.children_cap := zero;
//	};
//	ast_add_child(ref unop_node, deref expr_node, global_arena);
//	return rule_result
//	{
//		.tag := rule.reduce;
//		.reduce := reduce_replace_all_with(unop_node);
//	};
//};

rule_token_x_expr_x2unop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we've got a token of some type
	// we expect it to be a valid unop
	token_node ::= nodes # 0;
	expr_node ::= nodes # 1;

	tok ::= token_node->utok.tok.tok;

	op : ast_unop mut := zero;
	// make sure the token is a valid unop
	i : u64 mut;
	for(i = 0, i < __countof(ast_unop), i = i + 1)
	{
		if(deref(ast_unop_tokens # (i + 1)) == tok)
		{
			op = ((i + 1)@ast_unop);
		}
	}
	if(op == zero)
	{
		psyc_panic_begin(token_node->loc);
		putzstr("lex_token.");
		putzstr(__enumname(tok));
		putzstr(" is not a valid unary operator");
		psyc_diag_end();
		psyc_exit_failure();
	}
	unop_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.unop;
		.unop := op;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref unop_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(unop_node);
	};
};

rule_expr_x_token_x_expr_y2biop ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we've got a token of some type
	// we expect it to be a valid biop
	lhs_expr_node ::= nodes # 0;
	token_node ::= nodes # 1;
	rhs_expr_node ::= nodes # 2;

	tok ::= token_node->utok.tok.tok;

	op : ast_biop mut := zero;
	// make sure the token is a valid biop
	i : u64 mut;
	for(i = 0, i < __countof(ast_biop), i = i + 1)
	{
		if(deref(ast_biop_tokens # (i + 1)) == tok)
		{
			op = ((i + 1)@ast_biop);
		}
	}
	if(op == zero)
	{
		psyc_panic_begin(token_node->loc);
		putzstr("lex_token.");
		putzstr(__enumname(tok));
		putzstr(" is not a valid binary operator");
		psyc_diag_end();
		psyc_exit_failure();
	}
	biop_node ::= ast
	{
		.tag := ast_tag.expr;
		.expr := ast_expr_tag.biop;
		.biop := op;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref biop_node, deref lhs_expr_node, global_arena);
	ast_add_child(ref biop_node, deref rhs_expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(biop_node);
	};
};

rule_token_symbol2typename_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	name ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.typename;
			.typename := ast_typename
			{
				.tag := ast_typename_tag.wip;
				.quals := zero;
				.descriptor := ast_typename_descriptor.base;
				.basename := parse_state.src # (name.off);
				.basename_len := name.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_typename_wip2mut ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.mut));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.mut to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2static ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.static));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.static to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2weak ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	typename_quals ::= ref(typename_node->typename.quals);
	deref(typename_quals) = (deref(typename_quals) | (tyqual.weak));
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	// add tyqual.weak to the node
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2ptr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// make the current first node an underlying type of a new typename node (pointer)
	underlying ::= deref(nodes # 0);
	ptr_node ::= ast mut
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.wip;
			.quals := zero;
			.descriptor := ast_typename_descriptor.pointer;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref ptr_node, underlying, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ptr_node);
	};
};

rule_typename_wip2array_await_length ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	underlying ::= deref(nodes # 0);
	array_node ::= ast mut
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.array_await_length;
			.quals := zero;
			.descriptor := ast_typename_descriptor.array;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref array_node, underlying, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(array_node);
	};
};

rule_typename_array_await_length_expr_token_semicol2wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	array_length_expr ::= deref(nodes # 1);
	// add the array length as another child (typename_node should already have a child which is the underlying type)
	// then set the tag to wip (incase it has more stuff afterwards)...
	ast_add_child(typename_node, array_length_expr, global_arena);
	typename_node->typename.tag = (ast_typename_tag.wip);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_keyword_func_keyword_oparen2function_await_param_or_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// func(
	// make it into a function typename
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.typename;
			.typename := ast_typename
			{
				.tag := ast_typename_tag.function_await_param_or_arrow;
				.quals := zero;
				.descriptor := ast_typename_descriptor.function;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_typename_function_await_param_or_arrow_decl_finalised_token_arrow2function_await_return_type ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a function stem with a last param + an arrow
	// add the param decl as a child and then set tag to function_await_return_type
	typename_node ::= nodes # 0;
	decl_node ::= nodes # 1;
	ast_add_child(typename_node, deref decl_node, global_arena);
	typename_node->typename.tag = (ast_typename_tag.function_await_return_type);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_function_await_param_or_arrow_decl_finalised_token_comma2function_await_param_or_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a function stem with a param and then a comma
	// add the param decl as a child and then set tag to function_await_param_or_arrow
	// we obviously dont expect an arrow next but thats handled by an erroneous-case grammar_rule that has the arrow as a lookahead - in other words we dont deal with that error case in this rule.
	typename_node ::= nodes # 0;
	decl_node ::= nodes # 1;
	ast_add_child(typename_node, deref decl_node, global_arena);
	typename_node->typename.tag = (ast_typename_tag.function_await_param_or_arrow);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_typename_function_comma_followed_by_arrow ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	psyc_error_begin((nodes # 2)->loc);
	putzstr("in function typename, a ',' must be followed by another typename ");
	putchar(10);
	ast_print_annotated_source(parse_state.src, nodes # 2, nodes_count - 2);
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_typename_function_await_param_or_arrow_token_arrow2await_return_type ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_function_await_param_or_arrow arrow
	// make put it in the 'await return type' tag
	typename_node ::= nodes # 0;
	typename_node->typename.tag = (ast_typename_tag.function_await_return_type);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_function_await_return_type_typename_token_cparen2wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_function_await_return_type typename_finalised cparen
	// i.e a full function stem

	// we move it to wip instead of finalised (as function typenames can have qualifiers etc...)
	typename_node ::= nodes # 0;
	return_typename_node ::= nodes # 1;
	typename_node->typename.tag = (ast_typename_tag.wip);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	ast_add_child(typename_node, deref return_typename_node, global_arena);
	
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_typename_wip2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	typename_node ::= nodes # 0;
	// make the typename finalised.
	typename_node->typename.tag = (ast_typename_tag.finalised);
	typename_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon2decl_awaittype ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	name ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.decl;
			.decl := ast_decl
			{
				.tag := ast_decl_tag.await_type;
				.name := parse_state.src # (name.off);
				.name_len := name.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_decl_awaittype_typename_finalised2noinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// have a decl that awaits a typename
	// and a finalised typename
	// it's now a well-formed decl but without an init. we cant say the decl itself is finalised because we dont know if an initialisre is coming next so we will just say noinit for now.
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.noinit);
	ast_add_child(decl_node, deref(nodes # 1), global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_decl_noinit_token_initialiser2await_init_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// have a decl without an init followed by a :=
	// change its state to await_init_expr
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.await_init_expr);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_decl_noinit2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a decl that is marked noinit
	// but it definitely isnt followed by an initialiser
	// we just mark it as finalised
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.finalised);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon_token_initialiser2decl_await_init_expr ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we are 'foo ::='
	// we want to convert this to a decl with 'deduced' descriptor typename and the 'await_init_expr' tag
	// this is the same as 'foo : xyz :='
	// so we can share that code
	name ::= (nodes # 0)->utok.tok.lexeme;
	new_decl_node : ast mut := ast
	{
		.tag := ast_tag.decl;
		.decl := ast_decl
		{
			.tag := ast_decl_tag.await_init_expr;
			.name := parse_state.src # (name.off);
			.name_len := name.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	initialiser_node ::= (nodes # 2);
	// this is the typename
	ast_add_child(ref new_decl_node, ast
	{
		.tag := ast_tag.typename;
		.typename := ast_typename
		{
			.tag := ast_typename_tag.finalised;
			.descriptor := ast_typename_descriptor.deduced;
		};
		.begin_cursor := initialiser_node->begin_cursor;
		.end_cursor := initialiser_node->end_cursor;
		.loc := initialiser_node->loc;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	}, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(new_decl_node);
	};
};

rule_decl_await_init_expr_init_expr2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a decl that awaits an init expr
	// followed by a finalised expr that the grammar says is a valid initialiser
	// shove it as a child and call it a day
	decl_node ::= nodes # 0;
	decl_node->decl.tag = (ast_decl_tag.finalised);
	ast_add_child(decl_node, deref(nodes # 1), global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon_token_initialiser_token_keyword_struct_token_obrace2structure_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a bunch of tokens
	// turn them into an empty struct so we can stash it and add members
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.structure;
			.structure := ast_structure
			{
				.tag := ast_structure_tag.wip;
				.name := parse_state.src # (lexeme.off);
				.name_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_structure_wip_token_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a struct_wip and a cbrace
	// just set the struct as finalised and swallow everything else
	structure_node ::= nodes # 0;
	structure_node->structure.tag = (ast_structure_tag.finalised);
	structure_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_symbol_token_colon_token_initialiser_token_keyword_enum_token_obrace2enumeration_wip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a bunch of tokens
	// turn them into an empty enum so we can stash it and add members
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(ast
		{
			.tag := ast_tag.enumeration;
			.enumeration := ast_enumeration
			{
				.tag := ast_enumeration_tag.wip;
				.name := parse_state.src # (lexeme.off);
				.name_len := lexeme.len;
			};
			.children := zero;
			.children_count := zero;
			.children_cap := zero;
		});
	};
};

rule_enumeration_wip_token_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a enumeration_wip and a cbrace
	// just set the enum as finalised and swallow everything else
	enumeration_node ::= nodes # 0;
	enumeration_node->enumeration.tag = (ast_enumeration_tag.finalised);
	enumeration_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};


rule_decl_finalised_token_obrace2function_impl ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// decl_finalised {
	// this is the syntax for a function definition
	// we check the typename of the decl and ensure that is a function type
	decl_node ::= nodes # 0;
	typename_node ::= ast_get_child(decl_node, 0);
	if(typename_node->tag != (ast_tag.typename))
	{
		psyc_panic_begin(decl_node->loc);
		putzstr("internal error: first child of decl node was expected to be ast_tag.typename but it is ast_tag.");
		putzstr(__enumname(typename_node->tag));
		psyc_diag_end();
		psyc_exit_failure();
	}
	typename ::= typename_node->typename;
	if(typename.descriptor != (ast_typename_descriptor.function)) 
	{
		psyc_error_begin(decl_node->loc);
		putzstr("decl followed immediately by a '{' means a function definition, so i expect the decl typename to be a function type, but it is a ");
		putzstr(__enumname(typename.descriptor));
		putzstr(" type");
		psyc_diag_end();
		psyc_exit_failure();
	}
	decl ::= decl_node->decl;

	function_node ::= ast
	{
		.tag := ast_tag.function;
		.function := ast_function
		{
			.tag := ast_function_tag.impl;
			.name := decl.name;
			.name_len := decl.name_len;
			.is_extern := false;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref function_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(function_node);
	};
};

rule_decl_finalised_token_keyword_extern2function_finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// decl_finalised extern
	// this is the syntax for a function definition (extern)
	// we check the typename of the decl and ensure that is a function type
	decl_node ::= nodes # 0;
	typename_node ::= ast_get_child(decl_node, 0);
	if(typename_node->tag != (ast_tag.typename))
	{
		psyc_panic_begin(decl_node->loc);
		putzstr("internal error: first child of decl node was expected to be ast_tag.typename but it is ast_tag.");
		putzstr(__enumname(typename_node->tag));
		psyc_diag_end();
		psyc_exit_failure();
	}
	typename ::= typename_node->typename;
	if(typename.descriptor != (ast_typename_descriptor.function)) 
	{
		psyc_error_begin(decl_node->loc);
		putzstr("decl followed immediately by a '{' means a function definition, so i expect the decl typename to be a function type, but it is a ");
		putzstr(__enumname(typename.descriptor));
		putzstr(" type");
		psyc_diag_end();
		psyc_exit_failure();
	}
	decl ::= decl_node->decl;

	function_node ::= ast
	{
		.tag := ast_tag.function;
		.function := ast_function
		{
			.tag := ast_function_tag.finalised;
			.name := decl.name;
			.name_len := decl.name_len;
			.is_extern := true;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref function_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(function_node);
	};
};

rule_typename_finalised_token_obrace2blkinitwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// typename_finalised{
	// create a blkinitwip
	typename_node ::= nodes # 0;
	blkinit_node ::= ast
	{
		.tag := ast_tag.blkinitwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref blkinit_node, deref typename_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(blkinit_node);
	};
};

rule_blkinitwip_token_cbrace2blkinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	blkinitwip_node ::= nodes # 0;
	blkinitwip_node->tag = (ast_tag.expr);
	blkinitwip_node->expr = (ast_expr_tag.blkinit);
	blkinitwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_token_dot_token_symbol_token_initialiser2desiginitwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// .foo := 
	// turn into desiginitwip
	lexeme ::= (nodes # 0)->utok.tok.lexeme;
	desiginit_node ::= ast
	{
		.tag := ast_tag.desiginitwip;
		.desiginit := ast_desiginit
		{
			.name := parse_state.src # (lexeme.off);
			.name_len := lexeme.len;
		};
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(desiginit_node);
	};
};

rule_desiginitwip_expr_x2desiginit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// desiginitwip expr
	// turn it into a proper desiginit
	desiginitwip_node ::= nodes # 0;
	expr_node ::= nodes # 1;
	ast_add_child(desiginitwip_node, deref expr_node, global_arena);
	desiginitwip_node->tag = (ast_tag.desiginit);
	desiginitwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_expr_x_token_cast2castwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr@
	expr_node ::= nodes # 0;
	cast_node ::= ast
	{
		.tag := ast_tag.castwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref cast_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(cast_node);
	};
};

rule_function_impl_cbrace2finalised ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// function_impl }
	// means we are done and need to set our function to be in its finalised state
	function_node ::= nodes # 0;
	function_node->function.tag = (ast_function_tag.finalised);
	function_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_castwip_typename_finalised2cast ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// castwip typename_finalised
	// just add the typename_finalised as the 2nd child (first child should be the expr that we're casting)
	castwip_node ::= nodes # 0;
	typename_node ::= nodes # 1;
	if(castwip_node->children_count != 1)
	{
		psyc_panic_begin(castwip_node->loc);
		putzstr("invalid castwip state. should have 1 child (the expr to cast) but it had ");
		putuint(castwip_node->children_count);
		psyc_diag_end();
		psyc_exit_failure();
	}
	ast_add_child(castwip_node, deref typename_node, global_arena);
	castwip_node->tag = (ast_tag.expr);
	castwip_node->expr = (ast_expr_tag.cast);
	castwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_expr_x_token_dot2fieldwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr_x.
	// make into fieldwip
	expr_node ::= nodes # 0;
	field_node ::= ast
	{
		.tag := ast_tag.fieldwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref field_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(field_node);
	};
};

rule_fieldwip_expr_x2field ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// fieldwip expr
	// turn this into a proper field
	// fieldwip should already have a child 0 (the expr representing the struct/enum)
	fieldwip_node ::= nodes # 0;
	// remember this expr is expected to be either a symbol expr or a callfunc
	// in any case a symbol is easy to retrieve.
	expr_node ::= nodes # 1;
	ast_add_child(fieldwip_node, deref expr_node, global_arena);

	fieldwip_node->tag = (ast_tag.expr);
	fieldwip_node->expr = (ast_expr_tag.field);
	fieldwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};


rule_expr_x_token_arrow2ptrfieldwip ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// expr_x->
	// make into ptrfieldwip
	expr_node ::= nodes # 0;
	field_node ::= ast
	{
		.tag := ast_tag.ptrfieldwip;
		.children := zero;
		.children_count := zero;
		.children_cap := zero;
	};
	ast_add_child(ref field_node, deref expr_node, global_arena);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_replace_all_with(field_node);
	};
};

rule_ptrfieldwip_expr_x2ptrfield ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// ptrfieldwip expr
	// turn this into a proper ptrfield
	// ptrfieldwip should already have a child 0 (the expr representing the ptr)
	ptrfieldwip_node ::= nodes # 0;
	// remember this expr is expected to be either a symbol expr or a callfunc
	// in any case a symbol is easy to retrieve.
	expr_node ::= nodes # 1;
	ast_add_child(ptrfieldwip_node, deref expr_node, global_arena);

	ptrfieldwip_node->tag = (ast_tag.expr);
	ptrfieldwip_node->expr = (ast_expr_tag.ptrfield);
	ptrfieldwip_node->end_cursor = ((nodes # (nodes_count - 1))->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};


rule_expr_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have an expr (of any particular tag) and a semicol
	expr_node ::= nodes # 0;
	(expr_node->tag) = (ast_tag.stmt);
	(expr_node->stmt) = (ast_stmt_tag.expr);
	last ::= nodes # (nodes_count - 1);
	expr_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_function_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised function and a semicol
	function_node ::= nodes # 0;
	(function_node->tag) = (ast_tag.stmt);
	(function_node->stmt) = (ast_stmt_tag.function);
	last ::= nodes # (nodes_count - 1);
	function_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_desiginit_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a desiginit and a semicol
	desiginit_node ::= nodes # 0;
	(desiginit_node->tag) = (ast_tag.stmt);
	(desiginit_node->stmt) = (ast_stmt_tag.desiginit);
	last ::= nodes # (nodes_count - 1);
	desiginit_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_error_commit_non_desiginit_to_blkinit ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	stmt_node ::= nodes # 0;
	psyc_error_begin(stmt_node->loc);
	putzstr(__enumname(stmt_node->stmt));
	putzstr(" statement is not valid within a block initialiser - i will only accept designated initialisers.");
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

rule_decl_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised decl and a semicol
	decl_node ::= nodes # 0;
	(decl_node->tag) = (ast_tag.stmt);
	(decl_node->stmt) = (ast_stmt_tag.decl);
	last ::= nodes # (nodes_count - 1);
	decl_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_structure_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised structure and a semicol
	structure_node ::= nodes # 0;
	(structure_node->tag) = (ast_tag.stmt);
	(structure_node->stmt) = (ast_stmt_tag.structure);
	last ::= nodes # (nodes_count - 1);
	structure_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};

rule_enumeration_finalised_token_semicol2stmt ::= func(nodes : ast mut?, nodes_count : u64 -> rule_result)
{
	// we have a finalised enumeration and a semicol
	enumeration_node ::= nodes # 0;
	(enumeration_node->tag) = (ast_tag.stmt);
	(enumeration_node->stmt) = (ast_stmt_tag.enumeration);
	last ::= nodes # (nodes_count - 1);
	enumeration_node->end_cursor = (last->end_cursor);
	return rule_result
	{
		.tag := rule.reduce;
		.reduce := reduce_delete_all_but_first();
	};
};
