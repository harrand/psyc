x64_virtual_function : func(psy_fn : ir_val -> x64_val)
{
	// if .vfn, .addr is just the index of the function in psyir.program
	return x64_val
	{
		.tag := x64_val_tag.vfn;
		.addr := psy_fn.val@_;
	};
};

x64_virtual_memory_operand : func(psy_reg : ir_val, byte_offset : u64 -> x64_val)
{
	return x64_val
	{
		.tag := x64_val_tag.vmemreg;
		.reg := x64_virtual_reg_type(psy_reg.size);
		.addr := byte_offset;
	};
};

x64_virtual_reg_type : func(sz : irsz -> x64_reg)
{
	if(sz == irsz.q)
	{
		return x64_reg.rax;
	}
	if(sz == irsz.d)
	{
		return x64_reg.eax;
	}
	if(sz == irsz.w)
	{
		return x64_reg.ax;
	}
	if(sz == irsz.b)
	{
		return x64_reg.al;
	}
	if(sz == irsz.x)
	{
		return x64_reg.xmm4;
	}
	if(sz == irsz.y)
	{
		return x64_reg.xmm8;
	}
	return zero;
};

x64_virtual_reg : func(psy_reg : ir_val -> x64_val)
{
	// a virtual register is not a real x64 register
	// but a intermediate conversion between psyir and x64
	// basically a representation of the psyir register in a format that fits in a x64 instruction

	// any virtual register that appears as an operand in an instruction will get patched out and replaced with a concrete x64 register during register allocation (after the ir function instructions have fully been lowered)
	return x64_val
	{
		.tag := x64_val_tag.vreg;
		.reg := x64_virtual_reg_type(psy_reg.size);
		.addr := psy_reg.val@_;
	};
};

x64_ir_val : func(v : ir_val -> x64_val)
{
	if(v.tag == ir_valtag.imm)
	{
		return x64_imm(v.val);
	}
	if(v.tag == ir_valtag.r)
	{
		return x64_virtual_reg(v);
	}
	return zero;
};

x64_ir_stack : func(stack : ir -> x64_inst[2])
{
	// ir looks like this:
	// stack q0, 8

	// x64 looks like this:
	// sub rsp, 8
	// mov q0, rsp
	return x64_inst[2]
	{
		.0 := x64_inst
		{
			.tag := x64_inst_tag.sub;
			.operands := x64_val[4]
			{
				.0 := x64_register(x64_reg.rsp);
				.1 := x64_ir_val([stack.operands # 1]);
				... := zero;
			};
		};
		.1 := x64_inst
		{
			.tag := x64_inst_tag.mov;
			.operands := x64_val[4]
			{
				.0 := x64_virtual_reg([stack.operands # 0]);
				.1 := x64_register(x64_reg.rsp);
				... := zero;
			};
		};
	};
};

x64_ir_off : func(off : ir -> x64_inst[2])
{
	// probably ought to be a lea with a memory operand but an add+mov is simple enough for now
	return x64_inst[2]
	{
		.0 := x64_inst
		{
			.tag := x64_inst_tag.add;
			.operands := x64_val[4]
			{
				.0 := x64_virtual_reg([off.operands # 1]);
				.1 := x64_ir_val([off.operands # 2]);
				... := zero;
			};
		};
		.1 := x64_inst
		{
			.tag := x64_inst_tag.mov;
			.operands := x64_val[4]
			{
				.0 := x64_virtual_reg([off.operands # 0]);
				.1 := x64_ir_val([off.operands # 1]);
				... := zero;
			};
		};
	};
};

x64_ir_load : func(load : ir -> x64_inst)
{
	return x64_inst
	{
		.tag := x64_inst_tag.mov;
		.operands := x64_val[4]
		{
			.0 := x64_virtual_reg([load.operands # 0]);
			.1 := x64_virtual_memory_operand([load.operands # 1], 0);
			... := zero;
		};
	};
};

x64_ir_store : func(store : ir -> x64_inst)
{
	return x64_inst
	{
		.tag := x64_inst_tag.mov;
		.operands := x64_val[4]
		{
			.0 := x64_virtual_memory_operand([store.operands # 0], 0);
			.1 := x64_virtual_reg([store.operands # 1]);
			... := zero;
		};
	};
};

// sysv: 14 (rdi, rsi, rdx, rcx, r8, r9, xmm0-7)
// win64: 4 (rcx/xmm0, rdx/xmm1, r8/xmm2, r9/xmm3)
x64_max_param_regs ::= 14;

// x64_max_param_regs + 1 (mov for all params, and a call instruction)
x64_ir_call : func(call : ir, conv : x64_callconv -> x64_inst[15])
{
	ret : x64_inst mut[15] := zero;
	// + 1 for the call after the moves
	// emit all the moves we need (into the concrete destination registers we know we need)
	// in as many moves as it takes
	// assume fnval.tag == ir_valtag.fn
	//fn ::= [psyir.prog.fn # (fnval.val)];
	// ir offset is 1 because the first operand is the function (so first param is at 1)
	// x64 offset is 0 because the movs go in first
	cursor ::= x64_resolve_function_params_abi(ret # 0, call, 1, 0, conv);
	// now finally write to the call
	[ret # cursor] = x64_inst
	{
		.tag := x64_inst_tag.call;
		.operands := x64_val mut[4]
		{
			.0 := x64_virtual_function([call.operands # 0]);
		};
	};
	return ret;
};

// x64_max_param_regs + 1 (mov for all params, and a call instruction)
x64_ir_rcall : func(rcall : ir, conv : x64_callconv, ret_value : x64_val mut? -> x64_inst[15])
{
	// pretend the rcall is a call. same thing just that we handle rax/xmm0 etc after-the-fact depending on callconv
	ret ::= x64_ir_call(rcall, conv);
	// now we figure out which register the return value should be in.
	// fortunately the only 2 callconvs we support do largely the same thing
	// if the function returns floating point, its xmm0
	// otherwise, its rax
	fn ::= [psyir.prog.fn # [rcall.operands # 0].val];
	// check fn.ret as an irsz
	if((fn.ret == (irsz.x)) || (fn.ret == (irsz.y)))
	{
		// its floating point!
		[ret_value] = x64_register(x64_reg.xmm0);
	}
	else
	{
		[ret_value] = x64_register(x64_reg.rax);
	}
	return ret;
};

x64_resolve_function_params_abi : func(out_params : x64_inst mut?, call : ir, call_offset_ir : u64, call_offset_x64 : u64, conv : x64_callconv -> u64)
{
	// we have a call and it has some number of params
	// out_params # 0 will be the call address but the rest are for us to fill in
	// out_params # 1 for example should be a mov from the virtual register represented by call.operands # call_offset to the *concrete register* of this particular index as per the callconv
	if(conv == x64_callconv.sysv)
	{
		return x64_resolve_function_params_sysv(out_params, call, call_offset_ir, call_offset_x64);
	}
	if(conv == x64_callconv.win64)
	{
		return x64_resolve_function_params_win64(out_params, call, call_offset_ir, call_offset_x64);
	}
	// panic
	psyc_panic_begin(zero);
	putzstr("when lowering psyir to x86_64, got an unknwon calling convention ");
	putchar('"');
	putzstr(enumname(conv));
	putchar('"');
	putzstr(" (");
	putsint(conv@s64);
	putzstr(")");
	psyc_diag_end();
	psyc_exit_failure();
	return zero;
};

x64_resolve_function_params_sysv : func(out_params : x64_inst mut?, call : ir, call_offset_ir : u64, call_offset_x64 : u64 -> u64)
{
	psyc_panic_begin(zero);
	putzstr("calling convention sysv is not yet implemented");
	psyc_diag_end();
	psyc_exit_failure();
	// this is gonna be hard
	return zero;
};

x64_resolve_function_params_win64 : func(out_params : x64_inst mut?, call : ir, call_offset_ir : u64, call_offset_x64 : u64 -> u64)
{
	// ok so this is fairly simple
	// on x64 the params are:
	// rcx/xmm0, rdx/xmm1, r8/xmm2, r9/xmm3
	// choose xmmN if current operand is floating point
	// otherwise its the gpr
	win64_regs_limit ::= 4;
	// would love to make the sizes of these arrays win64_regs_limit (compiler bug)
	regs_not_fp ::= x64_reg[4]
	{
		.0 := x64_reg.rcx;
		.1 := x64_reg.rdx;
		.2 := x64_reg.r8;
		.3 := x64_reg.r9;
	};
	regs_fp ::= x64_reg[4]
	{
		.0 := x64_reg.xmm0;
		.1 := x64_reg.xmm1;
		.2 := x64_reg.xmm2;
		.3 := x64_reg.xmm3;
	};

	count ::= ir_operand_count(call);
	in_cursor : u64 mut := call_offset_ir;
	out_cursor : u64 mut := call_offset_x64;

	windows_x64_required_shadow ::= 32;
	stack_space_required : u64 mut := windows_x64_required_shadow;

	param_counter : u64 mut := 0;
	for(in_cursor = call_offset_ir; in_cursor < count; in_cursor = in_cursor + 1)
	{
		cur_ir ::= ir_get_operand(call, in_cursor);
		is_fp ::= (cur_ir.tag == ir_valtag.r) && ((cur_ir.size == irsz.x) || (cur_ir.size == irsz.y));
		if(param_counter >= win64_regs_limit)
		{
			stack_space_required = stack_space_required + 8;
		}
		else
		{
			out_reg : x64_reg mut := [regs_not_fp # param_counter];
			if(is_fp)
			{
				out_reg = [regs_fp # param_counter];
			}
			// write the result into out_params # cursor;
			[out_params # out_cursor] = x64_inst
			{
				.tag := x64_inst_tag.mov;
				.operands := x64_val[4]
				{
					// output concrete register
					.0 := x64_val
					{
						.tag := x64_val_tag.reg;
						.reg := out_reg;
					};
					// input ir value
					.1 := x64_ir_val(cur_ir);
				};
			};
			out_cursor = out_cursor + 1;
		}
		param_counter = param_counter + 1;
	}
	// now we allocate stack shadow space (32 bytes as required by win64 calling convention)
	// however it could be even more (multiple of 8 bytes though) if we had to pass some params on the stack
	if(stack_space_required > 0)
	{
		[out_params # out_cursor] = x64_inst
		{
			.tag := x64_inst_tag.sub;
			.operands := x64_val[4]
			{
				.0 := x64_register(x64_reg.rsp);
				.1 := x64_imm(stack_space_required);
			};
		};
		out_cursor = out_cursor + 1;

		// how many things do we need to copy onto the stack now
		stack_copy_count ::= param_counter - win64_regs_limit;
		i : u64 mut;
		for(i = win64_regs_limit; i < param_counter; i = i + 1)
		{
			[out_params # out_cursor] = x64_copy_8bytes_to_stack(windows_x64_required_shadow + (i - win64_regs_limit) * 8, x64_ir_val(ir_get_operand(call, call_offset_ir + i)));
			out_cursor = out_cursor + 1;
		}
	}
	return out_cursor;
};

x64_func_ir : func(fn : ir_func, conv : x64_callconv -> x64_func)
{
	return zero;
};
