Sleep ::= func(millis : s32) -> v0 := extern;

lex_slice ::= struct
{
	off : u64;
	len : u64;
};

impl_is_newline ::= func(char : u8) -> bool
{
	return char == 10;
};

impl_is_whitespace ::= func(char : u8) -> bool
{
	if(char == 0x20) // ' '
	{
		return true;
	}
	if(char == 0x0c) // '\f'
	{
		return true;
	}
	if(char == 0x0a) // '\n'
	{
		return true;
	}
	if(char == 0x0d) // '\r'
	{
		return true;
	}
	if(char == 0x09) // '/t'
	{
		return true;
	}
	if(char == 0x0b) // '/v'
	{
		return true;
	}
	return false;
};

lex_token ::= enum
{
	.comment := 0;
	.multicomment := 1;
	.integer_literal := 2;
	.decimal_literal := 3;
	.char_literal := 4;
	.string_literal := 5;
	.semicol := 6;
	.initialiser := 7;
	.colon := 8;
	.comma := 9;
	.dot := 10;
	.compare := 11;
	.comparen := 12;
	.assign := 13;
	.arrow := 14;
	.oparen := 15;
	.cparen := 16;
	.obrace := 17;
	.cbrace := 18;
	.obrack := 19;
	.cbrack := 20;
	.plus := 21;
	.dash := 22;
	.asterisk := 23;
	.fslash := 24;
	.cast := 25;
	.arr := 26;
	.bitwise_and := 27;
	.bitwise_or := 28;
	.bitwise_exor := 29;
	.modulo := 30;
	.invert := 31;
	.loreq := 32;
	.goreq := 33;
	.oanglebrack := 34;
	.canglebrack := 35;
	.keyword_static_if := 36;
	.keyword_if := 37;
	.keyword_else := 38;
	.keyword_while := 39;
	.keyword_for := 40;
	.keyword_return := 41;
	.keyword_yield := 42;
	.keyword_func := 43;
	.keyword_macro := 44;
	.keyword_extern := 45;
	.keyword_struct := 46;
	.keyword_enum := 47;
	.keyword_ref := 48;
	.keyword_deref := 49;
	.keyword_atomic_deref := 50;
	.keyword_defer := 51;
	.keyword_alias := 52;
	.keyword_at := 53;
	.keyword_true := 54;
	.keyword_false := 55;
	.keyword_zero := 56;
	.keyword_null := 57;
	.symbol := 58;
	.end_of_file := 59;
};

token_data ::= struct
{
	tok : lex_token;
	lexeme : lex_slice;
	begin : srcloc;
	end : srcloc;
};

lex_state ::= struct
{
	src : u8&;
	cursor : u64;
	line : u64;
	col : u64;

	tokens : token_data mut&;
	tokens_size : u64;
	tokens_cap : u64;
};

td_print ::= func(s : lex_state&, tok : token_data) -> v0
{
	slice ::= tok.lexeme;
	puts(__enumname(tok.tok));
	putchar(' ');
	lexeme_data ::= (s->src) at (slice.off);
	j : u64 mut;
	for(j = 0, j < (slice.len), j = j + 1)
	{
		putchar(deref(lexeme_data at j));
	}
	putchar(10);
};


ls_advance ::= func(s : lex_state mut&, count : u32) -> v0
{
	i : u32 mut;
	for(i = 0, i < count, i = i + 1)
	{
		is_newline ::= impl_is_newline(deref ((s->src) at i));
		if(is_newline)
		{
			(s->line) = (s->line) + 1;
			(s->col) = 0;
		}
		if(!is_newline)
		{
			(s->col) = (s->col) + 1;
		}
		(s->cursor) = (s->cursor) + 1;
	}
};

ls_advance_str ::= func(s : lex_state mut&, str : u8&) -> v0
{
	ls_advance(s, cstrlen(str)@_);
};

ls_push_token ::= func(state : lex_state mut&, token : token_data) -> v0
{
	if((state->tokens) == null)
	{
		psyc_panic(srcloc_current(), "lex_state did not initialise its token list correctly. it was null.");
	}
	if((state->tokens_size) >= (state->tokens_cap))
	{
		psyc_panic(srcloc_current(), "lex_state ran out of space for new tokens");
	}
	deref((state->tokens) at (state->tokens_size)) = token;
	(state->tokens_size) = (state->tokens_size) + 1;
	td_print(state, token);
};

tokenise_instruction ::= struct
{
	name : u8&;
	front_identifier : u8&;
	tokenise_fn : func(front : u8&, state : lex_state mut&) -> bool;
	trivial : bool;
	affects_code : bool;
	allow_run_on : bool;
};

[[private]]
lex_instructions : tokenise_instruction mut#59;

[[private]]
impl_setup_instruction ::= func(tok : lex_token, inst : tokenise_instruction mut) -> v0
{
	inst.name = __enumname(tok);
	deref(lex_instructions at (tok@s64)) = inst;
};

[[private]]
tokenise_invalid ::= func(front : u8&, state : lex_state mut&) -> bool
{
	psyc_panic(srcloc_current(), "asserty tokeniser function called. there is a bug in the lexer, or you didn't add support for a new lex token type.");
	return true;
};

[[private]]
tokenise_comment ::= func(front : u8&, state : lex_state mut&) -> bool
{
	// assume we already found '//' via front identifier.
	// just advance until we hit a newline.
	initial ::= state->cursor;
	chars_left : u64 mut := cstrlen(state->src) - (state->cursor);
	i : u64 mut;
	for(i = 0, i < chars_left, i = i + 1)
	{
		is_newline ::= impl_is_newline(deref(front at i));
		if(!is_newline)
		{
			ls_advance(state, 1);
		}
		if(is_newline)
		{
			chars_left = 0;
		}
	}
	ls_push_token(state, token_data
	{
		.tok := lex_token.comment;
		.lexeme := lex_slice{.off := initial; .len := i;};
		.begin := zero;
		.end := zero;
	});
	return true;
};

lex_setup ::= func() -> v0
{
	token_type_count ::= __sizeof(lex_instructions) / __sizeof(deref(lex_instructions at 0));
	i : u64 mut;

	// set them all to instructions that will assert.
	for(i = 0, i < token_type_count, i = i + 1)
	{
		deref(lex_instructions at i) = tokenise_instruction
		{
			.front_identifier := "";
			.tokenise_fn := tokenise_invalid;
			.trivial := false;
			.affects_code := false;
			.allow_run_on := false;
		};
	}

	// manually populate them all. if we choose one thats wrong then the assert will go off.

	impl_setup_instruction(lex_token.comment, tokenise_instruction
	{
		.front_identifier := "//";
		.tokenise_fn := tokenise_comment;
		.trivial := false;
		.affects_code := false;
		.allow_run_on := false;
	});
};

ls_verbose_print ::= func(s : lex_state&) -> v0
{
	puts(s->src);
	// just print out all token datas.
	i : u64 mut;
	for(i = 0, i < (s->tokens_size), i = i + 1)
	{
		tok ::= deref ((s->tokens) at i);
		td_print(s, tok);
	}
};

try_tokenise ::= func(state : lex_state mut&) -> bool
{
	token_type_count ::= __sizeof(lex_instructions) / __sizeof(deref(lex_instructions at 0));
	i : u64 mut;

	while(impl_is_whitespace(deref((state->src) at (state->cursor))))
	{
		(state->cursor) = (state->cursor) + 1;
	}
	if((state->cursor) >= cstrlen(state->src))
	{
		// we are out of chars to lex.
		return false;
	}
	front ::= (state->src) at (state->cursor);

	for(i = 0, i < token_type_count, i = i + 1)
	{
		instruction ::= deref(lex_instructions at i);
		if(instruction.trivial)
		{
			// its trivial. do trivial lexing.
			if(cstr_starts_with(front, instruction.front_identifier))
			{
				// match!
				ls_advance_str(state, instruction.front_identifier);
				// todo: not zero!
				ls_push_token(state, zero@token_data);
				return true;
			}
		}
		if(!(instruction.trivial))
		{
			// its not trivial. we must have a tokeniser_fn.
			tokfn ::= instruction.tokenise_fn;
			if(tokfn == null)
			{
				psyc_panic(srcloc_current(), "non-trivial instruction did not have a tokeniser function");
				__debugbreak();
			}
			if(cstr_starts_with(front, instruction.front_identifier))
			{
				tokfn(front, state);
				return true;
			}
		}
	}
	psyc_panic(srcloc_current(), "failed to lex");
	return true;
};

lex ::= func(src : u8&, a : arena mut&) -> lex_state
{
	psyc_timed(psyc_stage.lex);
	ret : lex_state mut := zero;
	(ret.src) = src;
	(ret.tokens_cap) = 1024;
	(ret.tokens) = arena_push(a, __sizeof(deref (ret.tokens)) * (ret.tokens_cap));
	len ::= cstrlen(src);

	while((ret.cursor) < len@_)
	{
		if(!try_tokenise(ref ret))
		{
			return ret;
		}
	}

	ls_verbose_print(ref ret);
	return ret;
};

== build ==
{
	add_source_file("diag.psy");
}
