Sleep ::= func(millis : s32) -> v0 := extern;

lex_slice ::= struct
{
	off : u64;
	len : u64;
};

impl_is_newline ::= func(char : u8) -> bool
{
	return char == 10;
};

impl_is_whitespace ::= func(char : u8) -> bool
{
	if(char == 0x20) // ' '
	{
		return true;
	}
	if(char == 0x0c) // '\f'
	{
		return true;
	}
	if(char == 0x0a) // '\n'
	{
		return true;
	}
	if(char == 0x0d) // '\r'
	{
		return true;
	}
	if(char == 0x09) // '/t'
	{
		return true;
	}
	if(char == 0x0b) // '/v'
	{
		return true;
	}
	return false;
};

impl_is_alpha ::= func(char : u8) -> bool
{
	if(char >= 'a')
	{
		if(char <= 'z')
		{
			return true;
		}
	}
	if(char >= 'A')
	{
		if(char <= 'Z')
		{
			return true;
		}
	}
	return false;
};

impl_starts_symbol ::= func(char : u8) -> bool
{
	if(impl_is_alpha(char))
	{
		return true;
	}
	if(char == '_')
	{
		return true;
	}
	return false;
};

impl_is_digit ::= func(char : u8) -> bool
{
	if(char >= '0')
	{
		if(char <= '9')
		{
			return true;
		}
	}
	return false;
};

impl_continues_symbol ::= func(char : u8) -> bool
{
	if(impl_is_digit(char))
	{
		return true;
	}
	if(char == '_')
	{
		return true;
	}
	if(impl_is_alpha(char))
	{
		return true;
	}
	return false;
};

impl_is_digit_or_period ::= func(char : u8) -> bool
{
	if(char == '.')
	{
		return true;
	}
	if(char >= '0')
	{
		if(char <= '9')
		{
			return true;
		}
	}
	return false;
};

impl_is_hexdigit ::= func(char : u8) -> bool
{
	if(impl_is_digit(char))
	{
		return true;
	}
	if(char >= 'a')
	{
		if(char <= 'f')
		{
			return true;
		}
	}
	if(char >= 'A')
	{
		if(char <= 'F')
		{
			return true;
		}
	}
	return false;
};

lex_token ::= enum
{
	.comment := 0;
	.multicomment := 1;
	.numeric_literal := 2;
	.symbol := 3;
	.char_literal := 4;
	.string_literal := 5;
	.semicol := 6;
	.initialiser := 7;
	.colon := 8;
	.comma := 9;
	.dot := 10;
	.compare := 11;
	.comparen := 12;
	.assign := 13;
	.arrow := 14;
	.oparen := 15;
	.cparen := 16;
	.obrace := 17;
	.cbrace := 18;
	.obrack := 19;
	.cbrack := 20;
	.plus := 21;
	.dash := 22;
	.asterisk := 23;
	.fslash := 24;
	.cast := 25;
	.arr := 26;
	.bitwise_and := 27;
	.bitwise_or := 28;
	.bitwise_exor := 29;
	.modulo := 30;
	.invert := 31;
	.loreq := 32;
	.goreq := 33;
	.oanglebrack := 34;
	.canglebrack := 35;
	.keyword_static_if := 36;
	.keyword_if := 37;
	.keyword_else := 38;
	.keyword_while := 39;
	.keyword_for := 40;
	.keyword_return := 41;
	.keyword_yield := 42;
	.keyword_func := 43;
	.keyword_macro := 44;
	.keyword_extern := 45;
	.keyword_struct := 46;
	.keyword_enum := 47;
	.keyword_ref := 48;
	.keyword_deref := 49;
	.keyword_atomic_deref := 50;
	.keyword_defer := 51;
	.keyword_alias := 52;
	.keyword_at := 53;
	.keyword_true := 54;
	.keyword_false := 55;
	.keyword_zero := 56;
	.keyword_null := 57;
};

token_data ::= struct
{
	tok : lex_token;
	lexeme : lex_slice;
	begin : srcloc;
	end : srcloc;
};

lex_state ::= struct
{
	path : u8&;
	src : u8&;
	cursor : u64;
	line : u64;
	col : u64;

	tokens : token_data mut&;
	tokens_size : u64;
	tokens_cap : u64;
};

ls_current_loc ::= func(s : lex_state&) -> srcloc
{
	return srcloc
	{
		.file := (s->path);
		.function := "";
		.line := (s->line);
		.column := (s->col);
	};
};

td_print ::= func(s : lex_state&, tok : token_data) -> v0
{
	slice ::= tok.lexeme;
	puts(__enumname(tok.tok));
	putchar(' ');
	lexeme_data ::= (s->src) at (slice.off);
	j : u64 mut;
	for(j = 0, j < (slice.len), j = j + 1)
	{
		putchar(deref(lexeme_data at j));
	}
	putchar(10);
};

ls_advance ::= func(s : lex_state mut&, count : u32) -> v0
{
	i : u32 mut;
	for(i = 0, i < count, i = i + 1)
	{
		is_newline ::= impl_is_newline(deref ((s->src) at i));
		if(is_newline)
		{
			(s->line) = (s->line) + 1;
			(s->col) = 0;
		}
		if(!is_newline)
		{
			(s->col) = (s->col) + 1;
		}
		(s->cursor) = (s->cursor) + 1;
	}
};

ls_advance_str ::= func(s : lex_state mut&, str : u8&) -> v0
{
	ls_advance(s, cstrlen(str)@_);
};

ls_push_token ::= func(state : lex_state mut&, token : token_data) -> v0
{
	if((state->tokens) == null)
	{
		psyc_panic(srcloc_current(), "lex_state did not initialise its token list correctly. it was null.");
	}
	if((state->tokens_size) >= (state->tokens_cap))
	{
		psyc_panic(srcloc_current(), "lex_state ran out of space for new tokens");
	}
	deref((state->tokens) at (state->tokens_size)) = token;
	(state->tokens_size) = (state->tokens_size) + 1;
	td_print(state, token);
};

tokenise_instruction ::= struct
{
	name : u8&;
	front_identifier : u8&;
	tokenise_fn : func(front : u8&, state : lex_state mut&) -> bool;
	trivial : bool;
	affects_code : bool;
	allow_run_on : bool;
};

[[private]]
lex_instructions : tokenise_instruction mut#59;

[[private]]
impl_setup_instruction ::= func(tok : lex_token, inst : tokenise_instruction mut) -> v0
{
	inst.name = __enumname(tok);
	deref(lex_instructions at (tok@s64)) = inst;
};

[[private]]
tokenise_invalid ::= func(front : u8&, state : lex_state mut&) -> bool
{
	psyc_panic(srcloc_current(), "asserty tokeniser function called. there is a bug in the lexer, or you didn't add support for a new lex token type.");
	return true;
};

[[private]]
tokenise_comment ::= func(front : u8&, state : lex_state mut&) -> bool
{
	// assume we already found '//' via front identifier.
	// just advance until we hit a newline.
	initial ::= state->cursor;
	chars_left : u64 mut := cstrlen(state->src) - (state->cursor);
	i : u64 mut;
	for(i = 0, i < chars_left, i = i + 1)
	{
		is_newline ::= impl_is_newline(deref(front at i));
		if(!is_newline)
		{
			ls_advance(state, 1);
		}
		if(is_newline)
		{
			chars_left = 0;
		}
	}
	ls_push_token(state, token_data
	{
		.tok := lex_token.comment;
		.lexeme := lex_slice{.off := initial; .len := i - 1;};
		.begin := zero;
		.end := zero;
	});
	return true;
};

[[private]]
tokenise_multicomment ::= func(front : u8&, state : lex_state mut&) -> bool
{
	// we're at /*
	// advance till we find '*/'
	initial ::= state->cursor;
	offset_till_close ::= cstr_find(front, "*/");
	if(offset_till_close == -1)
	{
		return false;
	}
	ls_advance(state, offset_till_close@_);
	ls_advance_str(state, "*/");
	ls_push_token(state, token_data
	{
		.tok := lex_token.multicomment;
		.lexeme := lex_slice{.off := initial; .len := offset_till_close@_;};
		.begin := zero;
		.end := zero;
	});
	return true;
};

[[private]]
tokenise_numeric_literal ::= func(front : u8&, state : lex_state mut&) -> bool
{
	myfront : u8& mut := front;
	is_hex_number ::= cstr_starts_with(front, "0x");
	f ::= deref(front at 0);
	starts_with_digit ::= impl_is_digit(f);
	starts_with_minus ::= f == '-';

	contains_decimal : bool mut := false;

	[[__force_mutable]]
	digit_checker : func(char : u8) -> bool := impl_is_digit_or_period;
	if(!is_hex_number)
	{
		if(!starts_with_digit)
		{
			if(!starts_with_minus)
			{
				return false;
			}
		}
	}

	initial ::= state->cursor;
	if(is_hex_number)
	{
		// skip the 0x
		myfront = (myfront at 2);
		ls_advance_str(state, "0x");
		// remember - if we're lexing a hex literal then we also allow a-fA-F in our digits.
		digit_checker = impl_is_hexdigit;
	}
	if(starts_with_minus)
	{
		myfront = (myfront at 1);
		ls_advance_str(state, "-");
	}
	found_decimal : bool mut := false;
	while(digit_checker(deref(myfront at 0)))
	{
		if(deref(myfront at 0) == '.')
		{
			if(found_decimal)
			{
				psyc_fatal_error(ls_current_loc(state), "multiple decimal points detected in numeric literal");
			}
			found_decimal = true;
		}
		myfront = (myfront at 1);
		ls_advance(state, 1);
	}
	ls_push_token(state, token_data
	{
		.tok := lex_token.numeric_literal;
		.lexeme := lex_slice{.off := initial; .len := ((state->cursor) - initial);};
		.begin := zero;
		.end := zero;
	});
	return true;
};

[[private]]
tokenise_symbol ::= func(front : u8&, state : lex_state mut&) -> bool
{
	myfront : u8& mut := front;
	if(!impl_starts_symbol(deref(myfront at 0)))
	{
		return false;
	}
	initial ::= state->cursor;
	ls_advance(state, 1);
	myfront = (myfront at 1);
	while(impl_continues_symbol(deref(myfront at 0)))
	{
		ls_advance(state, 1);
		myfront = (myfront at 1);
	}
	ls_push_token(state, token_data
	{
		.tok := lex_token.symbol;
		.lexeme := lex_slice{.off := initial; .len := ((state->cursor) - initial);};
		.begin := zero;
		.end := zero;
	});
	return true;
};

[[private]]
tokenise_char_literal ::= func(front : u8&, state : lex_state mut&) -> bool
{
	myfront : u8& mut := front;
	ls_advance(state, 1);
	myfront = (myfront at 1);
	// we're at '
	// go until next '
	initial ::= state->cursor;
	off ::= cstr_find(myfront, "'");
	if(off == -1)
	{
		psyc_fatal_error(ls_current_loc(state), "could not find terminating single-quote ' in char literal");
	}
	ls_advance(state, off + 1);
	charlen ::= ((state->cursor) - initial - 1);
	if(charlen == 0)
	{
		psyc_fatal_error(ls_current_loc(state), "invalid char literal ''");
	}
	ls_push_token(state, token_data
	{
		.tok := lex_token.char_literal;
		.lexeme := lex_slice{.off := initial; .len := charlen;};
		.begin := zero;
		.end := zero;
	});
	return true;
};

[[private]]
tokenise_string_literal ::= func(front : u8&, state : lex_state mut&) -> bool
{
	myfront : u8& mut := front;
	ls_advance(state, 1);
	myfront = (myfront at 1);
	// we're at '
	// go until next '
	initial ::= state->cursor;
	strlit ::= "\"";
	off ::= cstr_find(myfront, strlit);
	if(off == -1)
	{
		psyc_fatal_error(ls_current_loc(state), "could not find terminating double-quote in string literal");
	}
	ls_advance(state, off + 1);
	strlen ::= ((state->cursor) - initial - 1);
	ls_push_token(state, token_data
	{
		.tok := lex_token.string_literal;
		.lexeme := lex_slice{.off := initial; .len := strlen;};
		.begin := zero;
		.end := zero;
	});
	return true;
};

lex_setup ::= func() -> v0
{
	token_type_count ::= __sizeof(lex_instructions) / __sizeof(deref(lex_instructions at 0));
	i : u64 mut;

	// set them all to instructions that will assert.
	for(i = 0, i < token_type_count, i = i + 1)
	{
		deref(lex_instructions at i) = tokenise_instruction
		{
			.front_identifier := "";
			.tokenise_fn := tokenise_invalid;
			.trivial := false;
			.affects_code := false;
			.allow_run_on := false;
		};
	}

	// manually populate them all. if we choose one thats wrong then the assert will go off.

	impl_setup_instruction(lex_token.comment, tokenise_instruction
	{
		.front_identifier := "//";
		.tokenise_fn := tokenise_comment;
		.trivial := false;
		.affects_code := false;
		.allow_run_on := false;
	});

	impl_setup_instruction(lex_token.multicomment, tokenise_instruction
	{
		.front_identifier := "/*";
		.tokenise_fn := tokenise_multicomment;
		.trivial := false;
		.affects_code := true;
		.allow_run_on := false;
	});

	impl_setup_instruction(lex_token.numeric_literal, tokenise_instruction
	{
		.front_identifier := null;
		.tokenise_fn := tokenise_numeric_literal;
		.trivial := false;
		.affects_code := true;
		.allow_run_on := false;
	});

	impl_setup_instruction(lex_token.symbol, tokenise_instruction
	{
		.front_identifier := null;
		.tokenise_fn := tokenise_symbol;
		.trivial := false;
		.affects_code := true;
		.allow_run_on := false;
	});

	impl_setup_instruction(lex_token.char_literal, tokenise_instruction
	{
		.front_identifier := "'";
		.tokenise_fn := tokenise_char_literal;
		.trivial := false;
		.affects_code := true;
		.allow_run_on := false;
	});

	strlit ::= "\"";
	impl_setup_instruction(lex_token.string_literal, tokenise_instruction
	{
		.front_identifier := strlit;
		.tokenise_fn := tokenise_string_literal;
		.trivial := false;
		.affects_code := true;
		.allow_run_on := false;
	});
};

ls_verbose_print ::= func(s : lex_state&) -> v0
{
	puts(s->src);
	// just print out all token datas.
	i : u64 mut;
	for(i = 0, i < (s->tokens_size), i = i + 1)
	{
		tok ::= deref ((s->tokens) at i);
		td_print(s, tok);
	}
};

try_tokenise ::= func(state : lex_state mut&) -> bool
{
	token_type_count ::= __sizeof(lex_instructions) / __sizeof(deref(lex_instructions at 0));
	i : u64 mut;

	while(impl_is_whitespace(deref((state->src) at (state->cursor))))
	{
		(state->cursor) = (state->cursor) + 1;
	}
	if((state->cursor) >= cstrlen(state->src))
	{
		// we are out of chars to lex.
		return false;
	}
	front ::= (state->src) at (state->cursor);

	for(i = 0, i < token_type_count, i = i + 1)
	{
		instruction ::= deref(lex_instructions at i);
		if(instruction.trivial)
		{
			// its trivial. do trivial lexing.
			if(cstr_starts_with(front, instruction.front_identifier))
			{
				// match!
				ls_advance_str(state, instruction.front_identifier);
				// todo: not zero!
				ls_push_token(state, zero@token_data);
				return true;
			}
		}
		if(!(instruction.trivial))
		{
			// its not trivial. we must have a tokeniser_fn.
			tokfn ::= instruction.tokenise_fn;
			if(tokfn == null)
			{
				psyc_panic(srcloc_current(), "non-trivial instruction did not have a tokeniser function");
				__debugbreak();
			}
			if((instruction.front_identifier) == null)
			{
				if(tokfn(front, state))
				{
					return true;
				}
			}
			if((instruction.front_identifier) != null)
			{
				if(cstr_starts_with(front, instruction.front_identifier))
				{
					tokfn(front, state);
					return true;
				}
			}
		}
	}
	psyc_panic(srcloc_current(), "failed to lex");
	return true;
};

lex ::= func(path : u8&, src : u8&, a : arena mut&) -> lex_state
{
	psyc_timed(psyc_stage.lex);
	ret : lex_state mut := zero;
	(ret.path) = path;
	(ret.src) = src;
	(ret.tokens_cap) = 1024;
	(ret.tokens) = arena_push(a, __sizeof(deref (ret.tokens)) * (ret.tokens_cap));
	len ::= cstrlen(src);

	while((ret.cursor) < len@_)
	{
		if(!try_tokenise(ref ret))
		{
			return ret;
		}
	}

	ls_verbose_print(ref ret);
	return ret;
};

== build ==
{
	add_source_file("diag.psy");
}
